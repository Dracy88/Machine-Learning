{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Exploring the Dataset</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.1 Understanding the Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisi degli attributi:\n",
    "- preg = Pregnancies (Number of times pregnant)\n",
    "- plas = Glucose (Plasma glucose concentration a 2 hours in an oral glucose tolerance test)\n",
    "- pres = Blood pressure (Diastolic blood pressure (mm Hg))\n",
    "- skin = SkinThickness (Triceps skin fold thickness (mm))\n",
    "- test = Insulinic test (2-Hour serum insulin (mu U/ml))\n",
    "- mass = BMI (Body mass index (weight in kg/(height in m)^2))\n",
    "- pedi = Diabetes pedigree function\n",
    "- age = Age (years)\n",
    "- class = Class variable (0=no diabete or 1=diabete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.2 Loading Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "data = read_csv('dataset/pima-indians-diabetes.csv', names=names, header=0) # Carica il dataset ignorando la prima riga del header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.3 Peeking Data</h2>\n",
    "\n",
    "Rows:\n",
    "<ul>\n",
    "  <li>Too many --> algorithms may take too long to train</li>\n",
    "  <li>Too few --> perhaps you do not have enough data to train the algorithms</li>\n",
    "</ul>  \n",
    "Features:\n",
    "<ul>\n",
    "  <li>Too many -->some algorithms can be distracted or suffer poor performance due to the curse of dimensionality.</li>\n",
    "  <li>Too few --> perhaps aren't enough to train the algorithms</li>\n",
    "</ul> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il dataset è composto da 768 righe e 9 colonne\n",
      "   preg  plas  pres  skin  test  mass   pedi  age  class\n",
      "0     6   148    72    35     0  33.6  0.627   50      1\n",
      "1     1    85    66    29     0  26.6  0.351   31      0\n",
      "2     8   183    64     0     0  23.3  0.672   32      1\n",
      "3     1    89    66    23    94  28.1  0.167   21      0\n",
      "4     0   137    40    35   168  43.1  2.288   33      1\n",
      "5     5   116    74     0     0  25.6  0.201   30      0\n",
      "6     3    78    50    32    88  31.0  0.248   26      1\n",
      "7    10   115     0     0     0  35.3  0.134   29      0\n",
      "8     2   197    70    45   543  30.5  0.158   53      1\n",
      "9     8   125    96     0     0   0.0  0.232   54      1\n"
     ]
    }
   ],
   "source": [
    "''' Analisi primarie '''\n",
    "print(\"Il dataset è composto da\", data.shape[0], \"righe e\", data.shape[1], \"colonne\")\n",
    "print(data.head(10)) # Stampa le prime 10 righe dell'intero dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.4 Statistics</h2>\n",
    "\n",
    "We can use some statistics technique to understand better our datas.<br>\n",
    "Ask why are you seeing those specific numbers and think about how the numbers relate to the problem domain in general and specific entities that observations relate to.<br>\n",
    "Keep a small text file or note pad and jot down all of the ideas for how variables may relate, for what numbers mean, and ideas for techniques to try later. The things you write down now while the data is fresh will be very valuable later when you are trying to think up new things to try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.4.1 Descriptive Statistics</h3>\n",
    "\n",
    "Descriptive statistics can give you great insight into the shape of each attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          preg     plas     pres     skin     test     mass     pedi      age    class\n",
      "count  768.000  768.000  768.000  768.000  768.000  768.000  768.000  768.000  768.000\n",
      "mean     3.845  120.895   69.105   20.536   79.799   31.993    0.472   33.241    0.349\n",
      "std      3.370   31.973   19.356   15.952  115.244    7.884    0.331   11.760    0.477\n",
      "min      0.000    0.000    0.000    0.000    0.000    0.000    0.078   21.000    0.000\n",
      "25%      1.000   99.000   62.000    0.000    0.000   27.300    0.244   24.000    0.000\n",
      "50%      3.000  117.000   72.000   23.000   30.500   32.000    0.372   29.000    0.000\n",
      "75%      6.000  140.250   80.000   32.000  127.250   36.600    0.626   41.000    1.000\n",
      "max     17.000  199.000  122.000   99.000  846.000   67.100    2.420   81.000    1.000\n"
     ]
    }
   ],
   "source": [
    "from pandas import set_option\n",
    "set_option('display.width', 100) # Rende più visibile il risultato\n",
    "set_option('precision', 3) # Arrotonda alla 3° cifra decimale\n",
    "\n",
    "print(data.describe()) # Calcola la media, std, min, max e percentile di ciascun attributo\n",
    "# print(data[['preg', 'skin']].describe()) # Calcola le statistiche su attributi specifici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.4.2 Analyze Class Distribution (Classification Only)</h3>\n",
    "\n",
    "On classification problems you need to know how balanced the class values are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "0    0.651\n",
      "1    0.349\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(data.groupby('class').size() / data.shape[0]) # Mostra la distribuzione di ciascuna class label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.4.3 Feature Correlation</h3>\n",
    "\n",
    "Some machine learning algorithms like linear and logistic regression can suffer poor performance if there are highly correlated attributes in your dataset.<br>\n",
    "We can use the Pearson’s Correlation Coefficient, that assumes a normal distribution of the attributes involved. A correlation of -1 or 1 shows a full negative or positive correlation respectively, whereas a value of 0 shows no correlation at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        preg   plas   pres   skin   test   mass   pedi    age  class\n",
      "preg   1.000  0.129  0.141 -0.082 -0.074  0.018 -0.034  0.544  0.222\n",
      "plas   0.129  1.000  0.153  0.057  0.331  0.221  0.137  0.264  0.467\n",
      "pres   0.141  0.153  1.000  0.207  0.089  0.282  0.041  0.240  0.065\n",
      "skin  -0.082  0.057  0.207  1.000  0.437  0.393  0.184 -0.114  0.075\n",
      "test  -0.074  0.331  0.089  0.437  1.000  0.198  0.185 -0.042  0.131\n",
      "mass   0.018  0.221  0.282  0.393  0.198  1.000  0.141  0.036  0.293\n",
      "pedi  -0.034  0.137  0.041  0.184  0.185  0.141  1.000  0.034  0.174\n",
      "age    0.544  0.264  0.240 -0.114 -0.042  0.036  0.034  1.000  0.238\n",
      "class  0.222  0.467  0.065  0.075  0.131  0.293  0.174  0.238  1.000\n"
     ]
    }
   ],
   "source": [
    "print(data.corr(method='pearson'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.4.4 Skew of Univariate Distributions</h3>\n",
    "\n",
    "Skew refers to a distribution that is assumed Gaussian (normal or bell curve) that is shifted or squashed in one direction or another.<br>\n",
    "Many machine learning algorithms assume a Gaussian distribution.<br>\n",
    "Knowing that an attribute has a skew may allow you to perform data preparation to correct the skew and later improve the accuracy of your models.<br>\n",
    "<ul>\n",
    "  <li>Positive --> right skew</li>\n",
    "  <li>Negative --> left skew</li>\n",
    "  <li>Closer to zero --> less / symmetric skew</li>\n",
    "</ul>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preg     0.902\n",
      "plas     0.174\n",
      "pres    -1.844\n",
      "skin     0.109\n",
      "test     2.272\n",
      "mass    -0.429\n",
      "pedi     1.920\n",
      "age      1.130\n",
      "class    0.635\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(data.skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.4.5 Univariate Plots</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1.4.5.1 Histograms</h4>\n",
    "<p>A fast way to get an idea of the distribution of each attribute is to look at histograms.<br>\n",
    "From the shape of the bins you can quickly get a feeling for whether an attribute is Gaussian, skewed or even has an exponential distribution.</p>\n",
    "<p>It can also help you see possible outliers.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "data.hist()\n",
    "pyplot.show()\n",
    "\n",
    "# Notice how the attributes age, pedi and test may have an exponential distribution. \n",
    "# We can also see that perhaps the mass and pres and plas attributes may have a Gaussian or nearly Gaussian distribution. \n",
    "# This is interesting because many machine learning techniques assume a Gaussian univariate distribution on the input variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1.4.5.2 Density Plots</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(kind='density', subplots=True, layout=(3,3), sharex=False)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Pre-processing</h1>\n",
    "\n",
    "<p>Many machine learning algorithms make assumptions about your data.<br>\n",
    "It is often a very good idea to prepare your data in such way to best expose the structure of the problem to the machine learning algorithms that you intend to use.</p>\n",
    "<p>I would recommend creating many different views and transforms of your data, then exercise a handful of algorithms on each view of your dataset. This will help you to flush out which data transforms might be better at exposing the structure of your problem in general.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.1 Rescaling</h2>\n",
    "<p>When your data is comprised of attributes with varying scales, many machine learning algorithms can benefit from rescaling the attributes to all have the same scale (usually between 0 and 1).</p>\n",
    "<p>This is useful for optimization algorithms used in the core of machine learning algorithms like gradient descent. It is also useful for algorithms that weight inputs like regression and neural networks and algorithms that use distance measures like k-Nearest Neighbors.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.353 0.744 0.59  0.354 0.    0.501 0.234 0.483]\n",
      " [0.059 0.427 0.541 0.293 0.    0.396 0.117 0.167]\n",
      " [0.471 0.92  0.525 0.    0.    0.347 0.254 0.183]\n",
      " [0.059 0.447 0.541 0.232 0.111 0.419 0.038 0.   ]\n",
      " [0.    0.688 0.328 0.354 0.199 0.642 0.944 0.2  ]\n",
      " [0.294 0.583 0.607 0.    0.    0.382 0.053 0.15 ]\n",
      " [0.176 0.392 0.41  0.323 0.104 0.462 0.073 0.083]\n",
      " [0.588 0.578 0.    0.    0.    0.526 0.024 0.133]\n",
      " [0.118 0.99  0.574 0.455 0.642 0.455 0.034 0.533]\n",
      " [0.471 0.628 0.787 0.    0.    0.    0.066 0.55 ]]\n"
     ]
    }
   ],
   "source": [
    "#The Fit and Multiple Transform method is the preferred approach. You call the fit() function to prepare the parameters of the transform once on your data. Then later you can use the transform() function on the same data to prepare it for modeling and again on the test or validation dataset or new data that you may see in the future. The Combined Fit-And-Transform is a convenience that you can use for one off tasks.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import set_printoptions\n",
    "\n",
    "array = data.values # Converte i valori del dataframe in una matrice di dati\n",
    "X = array[:, 0:8] # Carica tutte le righe dei primi 8 attributi\n",
    "Y = array[:, 8] # Carica tutte le righe dell'ultimo attributo (la class label)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) # Imposta lo scalatore MinMax (con range 0-1)\n",
    "rescaledX = scaler.fit_transform(X) # Effettua il rescale su X\n",
    "\n",
    "set_printoptions(precision=3) # Imposta una precisione a 3 per numpy (sklearn si basa su numpy) per una migliore lettura\n",
    "print(rescaledX[0:10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.2 Standardize Data</h2>\n",
    "<p>Standardization is a useful technique to transform attributes (with a Gaussian distribution and differing means and standard deviations) to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1.</p>\n",
    "<p>It is most suitable for techniques that assume a Gaussian distribution in the input variables and work better with rescaled data, such as linear regression, logistic regression and linear discriminate analysis.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.64   0.848  0.15   0.907 -0.693  0.204  0.468  1.426]\n",
      " [-0.845 -1.123 -0.161  0.531 -0.693 -0.684 -0.365 -0.191]\n",
      " [ 1.234  1.944 -0.264 -1.288 -0.693 -1.103  0.604 -0.106]\n",
      " [-0.845 -0.998 -0.161  0.155  0.123 -0.494 -0.921 -1.042]\n",
      " [-1.142  0.504 -1.505  0.907  0.766  1.41   5.485 -0.02 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X) # Imposta lo scalatore standard\n",
    "standardizedX = scaler.transform(X) # Effettua la standardizzazione su X\n",
    "\n",
    "set_printoptions(precision=3) # Imposta una precisione a 3 per numpy (sklearn si basa su numpy) per una migliore lettura\n",
    "print(standardizedX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.3 Normalize Data</h2>\n",
    "<p>We can also rescaling each observation (row) to have a length of 1 (called a unit norm or a vector with the length of 1 in linear algebra).</p>\n",
    "<p>This pre-processing method can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using algorithms that weight input values such as neural networks and algorithms that use distance measures such as k-Nearest Neighbors.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.034 0.828 0.403 0.196 0.    0.188 0.004 0.28 ]\n",
      " [0.008 0.716 0.556 0.244 0.    0.224 0.003 0.261]\n",
      " [0.04  0.924 0.323 0.    0.    0.118 0.003 0.162]\n",
      " [0.007 0.588 0.436 0.152 0.622 0.186 0.001 0.139]\n",
      " [0.    0.596 0.174 0.152 0.731 0.188 0.01  0.144]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "scaler = Normalizer().fit(X) # Imposta lo scalatore normale\n",
    "normalizedX = scaler.transform(X) # Effettua la normalizzazione su X\n",
    "\n",
    "set_printoptions(precision=3) # Imposta una precisione a 3 per numpy (sklearn si basa su numpy) per una migliore lettura\n",
    "print(normalizedX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.4 Binarize Data</h2>\n",
    "<p>We can transform the data such as all values above the threshold are marked 1 and all equal to or below are marked as 0.</p>\n",
    "<p>It can be useful when we have probabilities that we want to make crisp values. <br>\n",
    "It is also useful when feature engineering and you want to add new features that indicate something meaningful.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "binarizer = Binarizer(threshold=0.5).fit(X) # Imposta la binarizzazione con soglia > a 0.5 --> 1\n",
    "binaryX = binarizer.transform(X) # Effettua la binarizzazione su X\n",
    "\n",
    "set_printoptions(precision=3) # Imposta una precisione a 3 per numpy (sklearn si basa su numpy) per una migliore lettura\n",
    "print(binaryX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Feature Selection</h1>\n",
    "<p>The data features used to train our machine learning models have a huge influence on the performance we can achieve.<br>\n",
    "Irrelevant or partially relevant features can negatively impact model performance.</p>\n",
    "\n",
    "<p>Feature selection is a process where we automatically select those features in our data that contribute most to the prediction variable or output in which you are interested. <br>\n",
    "Having irrelevant features in our data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression.</p>\n",
    "\n",
    "Pro:\n",
    "  <li>Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.</li>\n",
    "  <li>Improves Accuracy: Less misleading data means modeling accuracy improves.</li>\n",
    "  <li>Reduces Training Time: Less data means that algorithms train faster.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.1 Univariate Selection</h2>\n",
    "<p>Statistical tests can be used to select those features that have the strongest relationship with the output variable.</p>\n",
    "<p>The scikit-learn library provides the <i>SelectKBest class</i> that can be used with a suite of different statistical tests to select a specific number of features.</p>\n",
    "    \n",
    "<p>The example below uses the chi-squared (chi2) statistical test for non-negative features to select 4 of the best features.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punteggi:\n",
      " [ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304] \n",
      "\n",
      "[[148.    0.   33.6  50. ]\n",
      " [ 85.    0.   26.6  31. ]\n",
      " [183.    0.   23.3  32. ]\n",
      " [ 89.   94.   28.1  21. ]\n",
      " [137.  168.   43.1  33. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "chi_test = SelectKBest(score_func=chi2, k=4).fit(X, Y) # Seleziona i 4 migliori attributi mediante il test di chi-quadro\n",
    "features = chi_test.transform(X) # Effettua il test di chi-quadro\n",
    "\n",
    "set_printoptions(precision=3) # Imposta una precisione a 3 per numpy (sklearn si basa su numpy) per una migliore lettura\n",
    "print(\"Punteggi:\\n\", chi_test.scores_, \"\\n\")\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.2 Recursive Feature Elimination</h2>\n",
    "<p>The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.<br>\n",
    "It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 3 5 6 1 1 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression() # Seleziona il modello di regressione logistica\n",
    "rfe = RFE(model, 3) # Imposta la RFE sul modello di regressione richiedendo i 3 migliori attributi\n",
    "fit = rfe.fit(X, Y)\n",
    "\n",
    "print(\"Num Features:\", fit.n_features_) # Visualizza il numero di feature selezionate\n",
    "print(\"Selected Features:\", fit.support_) # Visualizza le feature selezionate\n",
    "print(\"Feature Ranking:\", fit.ranking_) # Visualizza le feature in ordine di importanza (1 = feature già selezionata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.3 Principal Component Analysis</h2>\n",
    "<p>Principal Component Analysis (or PCA) uses linear algebra to transform the dataset into a compressed form (generally called data reduction technique).<br>\n",
    "A property of PCA is that we can choose the number of dimensions or principal components in the transformed result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [0.889 0.062 0.026]\n",
      "Components:\n",
      " [[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n",
      "   5.372e-04 -3.565e-03]\n",
      " [-2.265e-02 -9.722e-01 -1.419e-01  5.786e-02  9.463e-02 -4.697e-02\n",
      "  -8.168e-04 -1.402e-01]\n",
      " [-2.246e-02  1.434e-01 -9.225e-01 -3.070e-01  2.098e-02 -1.324e-01\n",
      "  -6.400e-04 -1.255e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3) # Imposta la PCA con un numero di componenti pari a 3\n",
    "fit = pca.fit(X) # Applica la PCA ad X\n",
    "\n",
    "print(\"Explained Variance:\", fit.explained_variance_ratio_)\n",
    "print(\"Components:\\n\", fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.4 Feature Importance</h2>\n",
    "<p>Bagged decision trees (like Random Forest and Extra Trees) can be used to estimate the importance of features.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.112 0.204 0.096 0.078 0.083 0.156 0.123 0.148]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "model = ExtraTreesClassifier() # Seleziona l'ETC model\n",
    "model.fit(X, Y) # Applica l'ETC model ai dati\n",
    "\n",
    "print(model.feature_importances_) # Visualizza l'importanza degli attributi (più il singolo valoretende ad 1 più è importante)\n",
    "# In questo caso pare che gli attributi migliori risultino \"plas\" (con 0.242), \"age\" (con 0.138) and \"mass\" (con 0.136)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4. Evaluating ML Algorithms</h1>\n",
    "<p>We need to know how well our algorithms perform on unseen data.<br>\n",
    "The best way to evaluate the performance of an algorithm would be to make predictions for new data to which you already know the answers.</p>\n",
    "<p>The second best way is to use clever techniques from statistics called resampling methods that allow you to make accurate estimates for how well your algorithm will perform on new data.</p>\n",
    "\n",
    "We had four different techniques that we can use to split up our training dataset and create useful estimates of performance for our machine learning algorithms:\n",
    "    <li>Train and Test Sets</li>\n",
    "    <li>k-fold Cross Validation</li>\n",
    "    <li>Leave One Out Cross Validation</li>\n",
    "    <li>Repeated Random Test-Train Splits</li>\n",
    "\n",
    "Hint:\n",
    "<li>Using a train/test split is good for speed when using a slow algorithm and produces performance estimates with lower bias when using large datasets</li>\n",
    "<li>Techniques like leave-one-out cross validation and repeated random splits can be useful intermediates when trying to balance variance in the estimated performance, model training speed and dataset size</li>\n",
    "<li>The best advice is to experiment and find a technique for your problem that is fast and produces reasonable estimates of performance that you can use to make decisions</li>\n",
    "<li>If in doubt, use 10-fold cross validation</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4.1 Split into Train and Test Sets</h2>\n",
    "<p>The simplest method that we can use to evaluate the performance of a machine learning algorithm is to use different training and testing datasets.<br>\n",
    "We can take our original dataset and split it into two parts. Train the algorithm on the first part, make predictions on the second part and evaluate the predictions against the expected results.</p>\n",
    "\n",
    "<p>The size of the split can depend on the size and specifics of your dataset, although it is common to use 67% of the data for training and the remaining 33% for testing.</p>\n",
    "\n",
    "PRO:\n",
    "<ul>\n",
    "    <li>This algorithm evaluation technique is very fast (ideal for large datasets (millions of records) where there is strong evidence that both splits of the data are representative of the underlying problem)</ul>\n",
    "CONS:\n",
    "<ul>\n",
    "    <li>We can have a high variance (differences in the training and test dataset can result in meaningful differences in the estimate of accuracy)\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.591\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "test_size = 0.33 # Il test set rappresenta il 33% del dataset totale\n",
    "seed = 7 # Siccome la scelta dei campioni per il test set è casuale, fissiamo un seed per la riproducibilità dell'esperimento\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed) # Creazione dei train e test set\n",
    "\n",
    "model = LogisticRegression() # Selezione di un modello logistico\n",
    "model.fit(X_train, Y_train) # Applicazione del modello logistico ai dati di training\n",
    "\n",
    "result = model.score(X_test, Y_test) # Calcolo dell'accuracy\n",
    "print(\"Accuracy:\", round(result*100.0, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4.2 K-fold Cross Validation</h2>\n",
    "<p>It works by splitting the dataset into k-parts (e.g. k = 5 or k = 10), where each split of the data is called a fold.<br>\n",
    "The algorithm is trained on k − 1 folds with one held back and tested on the held back fold. This is repeated so that each fold of the dataset is given a chance to be the held back test set.<br>\n",
    "After running cross validation you end up with k different performance scores that you can summarize using a mean and a standard deviation</p>\n",
    "\n",
    "<p>The choice of k must allow the size of each test partition to be large enough to be a reasonable sample of the problem, whilst allowing enough repetitions of the train-test evaluation of the algorithm to provide a fair estimate of the algorithms performance on unseen data.</p><br>\n",
    "\n",
    "PRO:\n",
    "<li>Less variance than a single train-test set split</li>\n",
    "<li>The result is a more accurate and we had a reliable estimate of the performance of the algorithm on new data.</li>\n",
    "CONS:\n",
    "<li>More test implies more computation</li>\n",
    "\n",
    "<p>Hint: For modest sized datasets in the thousands or tens of thousands of records, k values of 3, 5 and 10 are common</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.086\n",
      "Dev. Std: 5.091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "num_folds = 10 # Imposta la divisione del dataset in 10 parti\n",
    "seed = 7\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True) # Imposta la kfold\n",
    "model = LogisticRegression() # Crea un modello logistico\n",
    "results = cross_val_score(model, X, Y, cv=kfold) # Applica la CV sul modello e sui dati\n",
    "\n",
    "print(\"Accuracy:\", round(results.mean()*100.0, 3))\n",
    "print(\"Dev. Std:\", round(results.std()*100.0, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4.3 Leave One Out Cross Validation</h2>\n",
    "<p>Setting k-fold = 1 is also called leave-one-out cross validation</p>\n",
    "<p>The result is a large number of performance measures that can be summarized in an effort to give a more reasonable estimate of the accuracy of your model on unseen data.</p>\n",
    "\n",
    "PRO:\n",
    "<li>Usefull when we had few datas</li>\n",
    "CONS:\n",
    "<li>Computationally expensive if we had a lot of data</li>\n",
    "\n",
    "\n",
    "Hint: Use it only when you had little datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.086\n",
      "Dev. Std: 5.091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "num_folds = 10 # Imposta la divisione del dataset in 10 parti\n",
    "\n",
    "loocv = LeaveOneOut() # Imposta la leave one out\n",
    "model = LogisticRegression() # Seleziona il modello logistico\n",
    "\n",
    "print(\"Accuracy:\", round(results.mean()*100.0, 3))\n",
    "print(\"Dev. Std:\", round(results.std()*100.0, 3))\n",
    "# Notare l'alta std rispetto alla tecnica precedente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4.4 Repeated Random Test-Train Splits</h2>\n",
    "<p>Another variation on k-fold cross validation is to create a random split of the data like the train/test split described above, but repeat the process of splitting and evaluation of the algorithm multiple times, like cross validation</p>\n",
    "\n",
    "PRO:\n",
    "<li>Speed of using a train/test split and the reduction in variance in the estimated performance of k-fold cross validation.</li>\n",
    "\n",
    "CONS:\n",
    "<li>Repetitions may include much of the same data in the train or the test split from run to run, introducing redundancy into the evaluation.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.97\n",
      "Dev. Std: 1.366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "n_splits = 10 # Imposta la divisione del dataset in 10 parti\n",
    "test_size = 0.3 # Il test set rappresenta il 33% del dataset totale\n",
    "seed = 7\n",
    "\n",
    "kfold = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed) # Imposta la shufflesplit\n",
    "model = LogisticRegression() # Seleziona il modello logistico\n",
    "results = cross_val_score(model, X, Y, cv=kfold) # Applica la CV sul modello e sui dati\n",
    "\n",
    "print(\"Accuracy:\", round(results.mean()*100.0, 3))\n",
    "print(\"Dev. Std:\", round(results.std()*100.0, 3))\n",
    "#Notiamo come abbia un'accuracy simile alla k-fold CV ma una bassissima dev. standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>5. Performance Metrics</h1>\n",
    "<p>The metrics that we choose to evaluate our machine learning algorithms are very important, because influences how the performance of machine learning algorithms is measured and compared.<br>\n",
    "This influence which algorithm to choose.</p>\n",
    "\n",
    "<p>We had 2 types of classification metrics:\n",
    "    <li>For classification purpose</li>\n",
    "    <li>For regression purpose</li>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5.1 Metrics for Classification Problems</h2>\n",
    "<p>The main classification metrics are:\n",
    "    <li>Classification Accuracy</li>\n",
    "    <li>Logarithmic Loss</li>\n",
    "    <li>Area Under ROC Curve</li>\n",
    "    <li>Confusion Matrix</li>\n",
    "    <li>Classification Report</li></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.1.1 Classification Accuracy</h3>\n",
    "<p>Classification accuracy is the number of correct predictions made as a ratio of all predictions made.<br>\n",
    "    This is the most common evaluation metric for classification problems, it is also the most misused.</p>\n",
    "    \n",
    "<p>Hint: Use only there are an equal number of observations in each class (which is rarely the case) and that all predictions and prediction errors are equally important (which is often not the case)</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.951\n",
      "Dev. Std: 4.841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7) # Imposta la kfold cv (k=10)\n",
    "model = LogisticRegression() # Seleziona il modello logistico\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy') # Applica la CV sul modello e sui dati e ne misura le performance mediante l'accuracy\n",
    "\n",
    "print(\"Accuracy:\", round(results.mean()*100.0, 3))\n",
    "print(\"Dev. Std:\", round(results.std()*100.0, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.1.2 Logarithmic Loss</h3>\n",
    "<p>Logarithmic loss (or logloss) is a performance metric for evaluating the predictions of probabilities of membership to a given class.<br>\n",
    "The scalar probability between 0 and 1 can be seen as a measure of confidence for a prediction by an algorithm.</p>\n",
    "<p>Predictions that are correct or incorrect are rewarded or punished proportionally to the confidence of the prediction.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss: -49.255\n",
      "Dev. Std: 4.701\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7) # Imposta la kfold cv (k=10)\n",
    "model = LogisticRegression() # Seleziona il modello logistico\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='neg_log_loss') # Applica la CV sul modello e sui dati e ne misura le performance mediante la logarithmic loss\n",
    "\n",
    "print(\"Logloss:\", round(results.mean()*100.0, 3))\n",
    "print(\"Dev. Std:\", round(results.std()*100.0, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.1.3 Area Under ROC Curve</h3>\n",
    "<p>Area under ROC Curve (or AUC for short) is a performance metric for binary classification problems.</p>\n",
    "<p>The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model that is as good as random.</p>\n",
    "<p>ROC can be broken down into sensitivity and specificity, so, a binary classification problem is really a trade-off between sensitivity and specificity, where:\n",
    "    <li>Sensitivity = true positive rate</li>\n",
    "    <li>Specificity = true negative rate</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 82.357\n",
      "Dev. Std: 4.084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7) # Imposta la kfold cv (k=10)\n",
    "model = LogisticRegression() # Seleziona il modello logistico\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='roc_auc') # Applica la CV sul modello e sui dati e ne misura le performance mediante la logarithmic loss\n",
    "\n",
    "print(\"AUC:\", round(results.mean()*100.0, 3))\n",
    "print(\"Dev. Std:\", round(results.std()*100.0, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.1.4 Confusion Matrix</h3>\n",
    "<p>The confusion matrix is a handy presentation of the accuracy of a model with two or more classes</p>\n",
    "<img src=\"images/ML/ML1.png\" alt=\"Confusion Matrix\" height=\"50%\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[141  21]\n",
      " [ 41  51]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression() # Seleziona il modello logistico\n",
    "model.fit(X_train, Y_train) # Carica i dati nel modello logistico\n",
    "\n",
    "predicted = model.predict(X_test) # Effettua le predizioni sul modello logistico sul test set\n",
    "matrix = confusion_matrix(Y_test, predicted) # Misura le performance generando una confusion matrix\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.1.5 Classification Report</h3>\n",
    "<p>The classification report displays the precision, recall, F1-score and support for each class.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.77      0.87      0.82       162\n",
      "        1.0       0.71      0.55      0.62        92\n",
      "\n",
      "avg / total       0.75      0.76      0.75       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression() # Seleziona il modello logistico\n",
    "model.fit(X_train, Y_train) # Carica i dati nel modello logistico\n",
    "\n",
    "predicted = model.predict(X_test) # Effettua le predizioni sul modello logistico sul test set\n",
    "report = classification_report(Y_test, predicted) # Genera i risultati di precision, recall, f1 e support\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5.2 Metrics for Regression Problems</h2>\n",
    "<p>The main classification metrics are:\n",
    "    <li>Mean Absolute Error</li>\n",
    "    <li>Mean Squared Error</li>\n",
    "    <li>R2</li>\n",
    "</p>\n",
    "\n",
    "n.b. For this metrics we will use a new dataset: Boston Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.2.1 Mean Absolute Error</h3>\n",
    "<p>The Mean Absolute Error (or MAE) is the sum of the absolute differences between predictions and actual values.</p><br>\n",
    "PRO:\n",
    "    <li>It gives an idea of how wrong the predictions were</li>\n",
    "\n",
    "CONS:\n",
    "    <li>It can't explain the direction of the error (e.g. over or under predicting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: -4.005\n",
      "Dev. Std: 2.084\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "names = ['Index', 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "'B', 'LSTAT', 'MEDV']\n",
    "dataframe = read_csv('dataset/Boston-full.csv', names=names, header=0) \n",
    "dataframe = dataframe.drop(columns=['Index'], axis=1) # Eliminiamo l'index dal dataset in quanto inutile ai fini della classificazione\n",
    "array = dataframe.values # Converte la struttura del dataframe in una matrice\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model = LinearRegression() # Seleziona il modello logistico\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_absolute_error') \n",
    "\n",
    "print(\"MAE:\", round(results.mean(), 3))\n",
    "print(\"Dev. Std:\", round(results.std(), 3))\n",
    "# A value of 0 indicates no error or perfect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.2.2 Mean Squared Error</h3>\n",
    "<p>The Mean Squared Error (or MSE) is much like the mean absolute error in that it provides a gross idea of the magnitude of error.</p>\n",
    "<p>Taking the square root of the mean squared error converts the units back to the original units of the output variable and can be meaningful for description and presentation (called the Root Mean Squared Error (or RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: -34.705\n",
      "Dev. Std: 45.574\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LinearRegression() # Seleziona il modello logistico\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(\"MSE:\", round(results.mean(), 3))\n",
    "print(\"Dev. Std:\", round(results.std(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5.2.3 R2</h3>\n",
    "<p>The R2 (or R Squared or Coefficient of Determination) metric provides an indication of the goodness of fit of a set of predictions to the actual values.<br>\n",
    "This is a value between 0 and 1 for no-fit and perfect fit respectively.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.203\n",
      "Dev. Std: 0.595\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LinearRegression() # Seleziona il modello logistico\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='r2')\n",
    "\n",
    "print(\"MSE:\", round(results.mean(), 3))\n",
    "print(\"Dev. Std:\", round(results.std(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>6. Spot-Checking</h1>\n",
    "<p>Spot-checking is a way of discovering which algorithms perform well on our machine learning problem, because we cannot know which algorithms are best suited to your problem beforehand.</p>\n",
    "<p>The question is: What algorithms should we spot-check on our dataset?<br>\n",
    "    We must trial a number of methods and focus attention on those that prove themselves the most promising</p>\n",
    "\n",
    "<p>Hint: trying a mixture of algorithms that might do well on our dataset and see what is good at picking out the structure in our data<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6.1 Strategy</h2>\n",
    "<ul>\n",
    "    <li>Try a mixture of algorithm representations (e.g. instances and trees)</li>\n",
    "    <li>Try a mixture of learning algorithms (e.g. different algorithms for learning the same type\n",
    "        of representation).</li>\n",
    "    <li>Try a mixture of modeling types (e.g. linear and nonlinear functions or parametric and nonparametric)</li>\n",
    "</ul>\n",
    "    <p>We will test now some of them</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6.2 Spot-Checking (for Classification Problems)</h2>\n",
    "<ul>\n",
    "    <li>Try a mixture of algorithm representations (e.g. instances and trees)</li>\n",
    "    <li>Try a mixture of learning algorithms (e.g. different algorithms for learning the same type\n",
    "        of representation).</li>\n",
    "    <li>Try a mixture of modeling types (e.g. linear and nonlinear functions or parametric and nonparametric)</li>\n",
    "</ul>\n",
    "    <p>We will test now some of them</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6.2.1 Linear Machine Learning Algorithms</h3>\n",
    "<p>This section demonstrates minimal recipes for how to use some linear ML algorithms</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.2.1.1 Logistic Regression</h4>\n",
    "<p>Logistic regression assumes a Gaussian distribution for the numeric input variables and can model binary classification problems.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7695146958304853\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv('dataset/pima-indians-diabetes.csv', names=names, header=0)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.2.1.2 Linear Discriminant Analysis</h4>\n",
    "<p>Linear Discriminant Analysis (or LDA) is a statistical technique for binary and multiclass classification. It too assumes a Gaussian distribution for the numerical input variables.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773462064251538\n"
     ]
    }
   ],
   "source": [
    "# LDA Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LinearDiscriminantAnalysis()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6.2.2 Nonlinear Machine Learning Algorithms</h3>\n",
    "<p>This section demonstrates minimal recipes for how to use some non linear ML algorithms</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.2.2.1 k-Nearest Neighbors</h4>\n",
    "<p>The k-Nearest Neighbors algorithm (or KNN) uses a distance metric to find the k most similar instances in the training data for a new instance and takes the mean outcome of the neighbors as the prediction.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7265550239234451\n"
     ]
    }
   ],
   "source": [
    "# KNN Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = KNeighborsClassifier()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.2.2.2 Naive Bayes</h4>\n",
    "<p>Naive Bayes calculates the probability of each class and the conditional probability of each class given each input value. These probabilities are estimated for new data and multiplied together, assuming that they are all independent (a simple or naive assumption).<br>\n",
    "When working with real-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for input variables using the Gaussian Probability Density Function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7551777170198223\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = GaussianNB()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.2.2.3 Classification and Regression Trees</h4>\n",
    "<p>Classification and Regression Trees (CART or just decision trees) construct a binary tree from the training data. Split points are chosen greedily by evaluating each attribute and each value of each attribute in the training data in order to minimize a cost function (like the Gini index).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6952153110047847\n"
     ]
    }
   ],
   "source": [
    "# CART Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = DecisionTreeClassifier()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.2.2.4 Support Vector Machines</h4>\n",
    "<p>Support Vector Machines (or SVM) seek a line that best separates two classes. Those data instances that are closest to the line that best separates the classes are called support vectors and influence where the line is placed. SVM has been extended to support multiple classes.<br>\n",
    "A powerful Radial Basis Function is used by default.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6510252904989747\n"
     ]
    }
   ],
   "source": [
    "# SVM Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = SVC()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6.3 Spot-Checking (for Regression Problems)</h2>\n",
    "<ul>\n",
    "    <li>Try a mixture of algorithm representations (e.g. instances and trees)</li>\n",
    "    <li>Try a mixture of learning algorithms (e.g. different algorithms for learning the same type\n",
    "        of representation).</li>\n",
    "    <li>Try a mixture of modeling types (e.g. linear and nonlinear functions or parametric and nonparametric)</li>\n",
    "</ul>\n",
    "    <p>We will test now some of them</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6.3.1 Linear Machine Learning Algorithms</h3>\n",
    "<p>This section demonstrates minimal recipes for how to use some linear ML algorithms</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.3.1.1 Linear Regression</h4>\n",
    "<p>Linear regression assumes that the input variables have a Gaussian distribution. It is also assumed that input variables are relevant to the output variable and that they are not highly correlated with each other (a problem called collinearity).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.705255944524986\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "names = ['Index', 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "'B', 'LSTAT', 'MEDV']\n",
    "dataframe = read_csv('dataset/Boston-full.csv', names=names, header=0) \n",
    "dataframe = dataframe.drop(columns=['Index'], axis=1) # Eliminiamo l'index dal dataset in quanto inutile ai fini della classificazione\n",
    "array = dataframe.values # Converte la struttura del dataframe in una matrice\n",
    "X = array[:,0:13]\n",
    "Y = array[:,13]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model = LinearRegression()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.3.1.2 Ridge Regression</h4>\n",
    "<p>Ridge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model measured as the sum squared value of the coefficient values (also called the L2-norm).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.07824620925932\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = Ridge()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.3.1.3 LASSO Regression</h4>\n",
    "<p>The Least Absolute Shrinkage and Selection Operator (or LASSO for short) is a modification of linear regression, like ridge regression, where the loss function is modified to minimize the complexity of the model measured as the sum absolute value of the coefficient values (also called the L1-norm).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.464084588302306\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = Lasso()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.3.1.4 ElasticNet Regressionn</h4>\n",
    "<p>ElasticNet is a form of regularization regression that combines the properties of both Ridge Regression and LASSO regression. It seeks to minimize the complexity of the regression model (magnitude and number of regression coefficients) by penalizing the model using both the L2-norm (sum squared coefficient values) and the L1-norm (sum absolute coefficient values).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-31.16457371424976\n"
     ]
    }
   ],
   "source": [
    "# ElasticNet Regression\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = ElasticNet()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6.3.2 Non Linear Machine Learning Algorithms</h3>\n",
    "<p>This section provides examples of how to use three different nonlinear machine learning algorithms for regression in Python with scikit-learn.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.3.2.1 K-Nearest Neighbors</h4>\n",
    "<p>The k-Nearest Neighbors algorithm (or KNN) locates the k most similar instances in the training dataset for a new data instance. From the k neighbors, a mean or median output variable is taken as the prediction. Of note is the distance metric used (the metric argument).</p>\n",
    "<p>The Minkowski distance is used by default, which is a generalization of both the Euclidean distance (used when all inputs have the same scale) and Manhattan distance (for when the scales of the input variables differ).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-107.28683898039215\n"
     ]
    }
   ],
   "source": [
    "# KNN Regression\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = KNeighborsRegressor()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.3.2.2 Classification and Regression Tree</h4>\n",
    "<p>Decision trees or the Classification and Regression Trees (CART as they are known) use the training data to select the best points to split the data in order to minimize a cost metric. The default cost metric for regression decision trees is the mean squared error, specified in the criterion parameter.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-39.45716549019608\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regression\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = DecisionTreeRegressor()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6.3.2.3 Support Vector Machines</h4>\n",
    "<p>Support Vector Machines (SVM) were developed for binary classification. The technique has been extended for the prediction real-valued problems called Support Vector Regression (SVR).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-91.04782433324428\n"
     ]
    }
   ],
   "source": [
    "# SVM Regression\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = SVR()\n",
    "scoring = 'neg_mean_squared_error'\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>7. Compare Machine Learning Algorithms</h1>\n",
    "<p>When we work on a machine learning project, we often end up with multiple good models to choose from.<br>It is important to compare the performance of multiple different machine learning algorithms consistently.</p>\n",
    "<p>We will see how:\n",
    "    <li>To formulate an experiment to directly compare machine learning algorithms.</li>\n",
    "    <li>To make a reusable template for evaluating the performance of multiple algorithms on one dataset.</li>\n",
    "    <li>To report and visualize the results when comparing algorithm performance.</li>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>7.1 Formulating an Experiment</h2>\n",
    "<br>The key to a fair comparison of machine learning algorithms is ensuring that each algorithm is evaluated in the same way on the same data.\n",
    "<li>Use a resampling methods like cross validation, so we can get an estimate for how accurate each model may be on unseen data.</li>\n",
    "<li>Use a number of different ways of looking at the estimated accuracy of our machine learning algorithms (e.g.  average accuracy, variance and other properties of the distribution of model accuracies).</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: LR results mean: 0.7695146958304853 results std: 0.04841051924567195\n",
      "name: LDA results mean: 0.773462064251538 results std: 0.05159180390446138\n",
      "name: KNN results mean: 0.7265550239234451 results std: 0.06182131406705549\n",
      "name: CART results mean: 0.705604921394395 results std: 0.06536131102456491\n",
      "name: NB results mean: 0.7551777170198223 results std: 0.04276593954064409\n",
      "name: SVM results mean: 0.6510252904989747 results std: 0.07214083485055327\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv('dataset/pima-indians-diabetes.csv', names=names, header=0)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(\"name:\", name, \"results mean:\", cv_results.mean(), \"results std:\", cv_results.std())\n",
    "\n",
    "# boxplot algorithm comparison\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>7.2 Workflow with Pipelines</h2>\n",
    "<p>There are standard workflows in a machine learning project that can be automated. With Pipelines we can clearly define and automate these workflows, allowing to make a linear sequence of data transforms to be chained together culminating in a modeling process that can be evaluated.</p>\n",
    "<p>The goal is to ensure that all of the steps in the pipeline are constrained to the data available for the evaluation, such as the training dataset or each fold of the cross validation procedure.</p>\n",
    "We will see how:\n",
    "<li>How to use pipelines to minimize data leakage.</li>\n",
    "<li>How to construct a data preparation and modeling pipeline</li>\n",
    "<li>How to construct a feature extraction and modeling pipeline.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>7.2.1 Data Preparation and Pipeline</h3>\n",
    "<p>An easy trap to fall into in applied machine learning is leaking data from our training dataset to our test dataset.</p>\n",
    "<p>To avoid this trap we need a robust test harness with strong separation of training and testing.<br>\n",
    "Data preparation is one easy way to leak knowledge of the whole training dataset to the algorithm.<br>\n",
    "For example, preparing our data using normalization or standardization on the entire training dataset before learning would not be a valid test because the training dataset would have been influenced by the scale of the data in the test set.</p>\n",
    "<p>Pipelines help you prevent data leakage in your test harness by ensuring that data preparation like standardization is constrained to each fold of your cross validation procedure.</p>\n",
    "<p>The pipeline is defined with two steps:\n",
    "    <ol>\n",
    "        <li>Standardize the data.</li>\n",
    "        <li>Learn a Linear Discriminant Analysis model.</li>\n",
    "    </ol>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773462064251538\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline that standardizes the data then creates a model\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# load data\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv('dataset/pima-indians-diabetes.csv', names=names, header=0)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('lda', LinearDiscriminantAnalysis()))\n",
    "\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "# evaluate pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean()) # Stampa la media dei risultati delle varie kfold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>7.2.2 Feature Extraction and Modeling Pipeline</h3>\n",
    "<p>Feature extraction is another procedure that is susceptible to data leakage.<br>\n",
    "Like data preparation, feature extraction procedures must be restricted to the data in your training dataset. The pipeline provides a handy tool called the FeatureUnion which allows the results of multiple feature selection and extraction procedures to be combined into a larger dataset on which a model can be trained. Importantly, all the feature extraction and the feature union occurs within each fold of the cross validation procedure.</p>\n",
    "<p>The pipeline defined with four steps:\n",
    "<ol>\n",
    "    <li>Feature Extraction with Principal Component Analysis</li>\n",
    "    <li>Feature Extraction with Statistical Selection</li>\n",
    "    <li>Feature Union.</li>\n",
    "    <li>Learn a Logistic Regression Model.</li>\n",
    "</ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7760423786739576\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline that extracts features from the data then creates a model\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# create feature union\n",
    "features = []\n",
    "features.append(('pca', PCA(n_components=3)))\n",
    "features.append(('select_best', SelectKBest(k=6)))\n",
    "feature_union = FeatureUnion(features)\n",
    "\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('feature_union', feature_union))\n",
    "estimators.append(('logistic', LogisticRegression()))\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "# evaluate pipeline\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>8. Improving Performance</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>8.1 Ensamble</h2>\n",
    "<p>Ensembles can give us a boost in accuracy on our dataset.</p>\n",
    "<p>This lesson will step you through Boosting, Bagging and Majority Voting and show you how you can continue to ratchet up the accuracy of the models on your own datasets</p>\n",
    "<p>The three most popular methods for combining the predictions from different models are:</p>\n",
    "<ol>\n",
    "    <li>Bagging: Building multiple models (typically of the same type) from different subsamples of the training dataset.</li>\n",
    "<li>Boosting: Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the sequence of models.</li>\n",
    "<li>Voting: Building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>8.1.1 Bagging Algorithms</h3>\n",
    "<p>Bootstrap Aggregation (or Bagging) involves taking multiple samples from your training dataset (with replacement) and training a model for each sample.</p>\n",
    "<p>The final output prediction is averaged across the predictions of all of the sub-models.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>8.1.1.1 Bagged Decision Trees</h4>\n",
    "<p>Bagging performs best with algorithms that have high variance (a popular example are decision trees, often constructed without pruning).</p>\n",
    "<p>In the example below we will use the BaggingClassifier with the Classification and Regression Trees algorithm.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.770745044429255\n"
     ]
    }
   ],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv('dataset/pima-indians-diabetes.csv', names=names, header=0)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>8.1.1.2 Random Forest</h4>\n",
    "<p>Random Forests is an extension of bagged decision trees.<br>\n",
    "Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of each tree, only a random subset of features are considered for each split.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7642344497607655\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "num_trees = 100\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=3)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>8.1.1.3 Extra Trees</h4>\n",
    "<p>Extra Trees are another modification of bagging where random trees are constructed from samples of the training dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7733937115516063\n"
     ]
    }
   ],
   "source": [
    "# Extra Trees Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "num_trees = 100\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>8.1.2 Boosting Algorithms</h3>\n",
    "<p>Boosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes of the models before them in the sequence. Once created, the models make predictions which may be weighted by their demonstrated accuracy and the results are combined to create a final output prediction. \n",
    "<p>The two most common boosting ensemble machine learning algorithms are:</p>\n",
    "<ul>\n",
    "    <li>AdaBoost</li>\n",
    "    <li>Stochastic Gradient Boosting</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>8.1.2.1 AdaBoost</h4>\n",
    "<p>AdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay or less attention to them in the construction of subsequent models.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.760457963089542\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "num_trees = 30\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>8.1.2.2 Stochastic Gradient Boosting</h4>\n",
    "<p>Stochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most sophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of the best techniques available for improving performance via ensembles.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7669002050580999\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Boosting Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "num_trees = 100\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>8.1.3 Voting Ensamble</h3>\n",
    "<p>Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms. It works by first creating two or more standalone models from your training dataset.</p>\n",
    "<p>A voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data.</p>\n",
    "<p>The predictions of the sub-models can be weighted, but specifying the weights for classifiers manually or even heuristically is difficult.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7708304853041694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yukina/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/yukina/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/yukina/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/yukina/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/yukina/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/yukina/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/yukina/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/yukina/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/yukina/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/yukina/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Voting Ensemble for Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "# create the sub models\n",
    "estimators = []\n",
    "estimators.append(('logistic', LogisticRegression()))\n",
    "estimators.append(('cart', LinearDiscriminantAnalysis()))\n",
    "estimators.append(('nb', GaussianNB()))\n",
    "\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>8.2 Improve Performance with Algorithm Tuning</h2>\n",
    "<p>Machine learning models are parameterized so that their behavior can be tuned for a given problem.</p>\n",
    "<p>Models can have many parameters and finding the best combination of parameters can be treated as a search problem.</p>\n",
    "<p>There are three main strategies:</p>\n",
    "<ul>\n",
    "    <li>How to use a grid search algorithm tuning strategy</li>\n",
    "    <li>How to use a random search algorithm tuning strategy.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>8.2.1 Grid Search Parameter Tuning</h3>\n",
    "<p>Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2796175593129722\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Grid Search for Algorithm Tuning\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "alphas = numpy.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "param_grid = dict(alpha=alphas)\n",
    "model = Ridge()\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid.fit(X, Y)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>8.2.2 Random Search Parameter Tuning</h3>\n",
    "<p>Random search is an approach to parameter tuning that will sample algorithm parameters from a random distribution (i.e. uniform) for a fixed number of iterations.</p>\n",
    "<p>A model is constructed and evaluated for each combination of parameters chosen. A total of 100 iterations are performed with uniformly random alpha values selected in the range between 0 and 1 (the range that alpha values can take).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27961712703051095\n",
      "0.9779895119966027\n"
     ]
    }
   ],
   "source": [
    "# Randomized for Algorithm Tuning\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {'alpha': uniform()}\n",
    "model = Ridge()\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100, random_state=7)\n",
    "rsearch.fit(X, Y)\n",
    "print(rsearch.best_score_)\n",
    "print(rsearch.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>9. Save and Load Machine Learning Models</h1>\n",
    "<p>Saving and load our machine learning model in Python using scikit-learn allows us to save your model to file and load it later in order to make predictions.</p>\n",
    "<p>There are two main methods to do it:</p>\n",
    "<ul>\n",
    "    <li>Pickle</li>\n",
    "    <li>Joblib</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>9.1 Pickle</h2>\n",
    "<p>Pickle is the standard way of serializing objects in Python.</p>\n",
    "<p>We can use it to serialize our machine learning algorithms and save the serialized format to a file. Later we can load this file to deserialize our model and use it to make new predictions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7559055118110236\n"
     ]
    }
   ],
   "source": [
    "# Save Model Using Pickle\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "# Fit the model on 33%\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.txt'\n",
    "dump(model, open(filename, 'wb'))\n",
    "\n",
    "# some time later...\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>9.2 Joblib</h2>\n",
    "<p>The Joblib library is part of the SciPy ecosystem and provides utilities for pipelining Python jobs.</p>\n",
    "<p>It provides utilities for saving and loading Python objects that make use of NumPy data structures, efficiently. This can be useful for some machine learning algorithms that require a lot of parameters or store the entire dataset (e.g. k-Nearest Neighbors).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7559055118110236\n"
     ]
    }
   ],
   "source": [
    "# Save Model Using joblib\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals.joblib import dump\n",
    "from sklearn.externals.joblib import load\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7) # Fit the model on 33%\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "dump(model, filename)\n",
    "\n",
    "# some time later...\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = load(filename)\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>9.3 Some Tips..</h2>\n",
    "<p>When finalizing our machine learning models, taking a note of version of what are we using allow us to re-create the environment if for some reason we cannot reload your model on another machine or another platform at a later time.</p>\n",
    "\n",
    "In particular:\n",
    "<ul>\n",
    "    <li>Take a note of python version</li>\n",
    "    <li>Take a note of library version</li>\n",
    "    <li>Manual Serialization: we might like to manually output the parameters of our learned model so that we can use them directly in scikit-learn or another platform in the future. Often the techniques used internally by machine learning algorithms to make predictions are a lot simpler than those used to learn the parameters can may be easy to implement in custom code that you have control over.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
